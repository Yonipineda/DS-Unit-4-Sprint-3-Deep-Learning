{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "with requests.get(url) as res:\n",
    "    res.encoding = 'utf-8'\n",
    "    data = res.text[553:5757527]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data into integers rather than raw text\n",
    "\n",
    "# unique chars \n",
    "chars = list(set(data))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many chars in data?\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode data using char_int\n",
    "encoded = [char_int[c] for c in data]\n",
    "len(encoded) == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286973, 286973)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into useable text sequences\n",
    "sequences = []\n",
    "next_char = []\n",
    "maxlen = 60\n",
    "step = 20\n",
    "\n",
    "for i in range(0, len(encoded)-maxlen, step):\n",
    "  curr_sequence = encoded[i:i+maxlen]\n",
    "  sequences.append(curr_sequence)\n",
    "  next_char.append(encoded[maxlen+i])\n",
    "\n",
    "len(sequences), len(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([105,\n",
       "  12,\n",
       "  85,\n",
       "  80,\n",
       "  105,\n",
       "  90,\n",
       "  11,\n",
       "  3,\n",
       "  16,\n",
       "  22,\n",
       "  80,\n",
       "  95,\n",
       "  80,\n",
       "  105,\n",
       "  103,\n",
       "  11,\n",
       "  57,\n",
       "  48,\n",
       "  38,\n",
       "  105,\n",
       "  11,\n",
       "  5,\n",
       "  105,\n",
       "  103,\n",
       "  36,\n",
       "  22,\n",
       "  22,\n",
       "  36,\n",
       "  8,\n",
       "  3,\n",
       "  105,\n",
       "  69,\n",
       "  85,\n",
       "  8,\n",
       "  48,\n",
       "  80,\n",
       "  38,\n",
       "  16,\n",
       "  80,\n",
       "  8,\n",
       "  57,\n",
       "  80,\n",
       "  24,\n",
       "  10,\n",
       "  24,\n",
       "  10,\n",
       "  2,\n",
       "  45,\n",
       "  95,\n",
       "  85,\n",
       "  11,\n",
       "  57,\n",
       "  52,\n",
       "  105,\n",
       "  103,\n",
       "  36,\n",
       "  22,\n",
       "  22,\n",
       "  36,\n",
       "  8],\n",
       " 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure a sequence looks as expected\n",
    "sequences[0], next_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((286973, 60, 107), (286973, 107))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape data into final format\n",
    "import numpy as np \n",
    "\n",
    "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "  for j, char in enumerate(sequence):\n",
    "    X[i, j, char] = 1\n",
    "  y[i, next_char[i]] = 1\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model imports \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               120832    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 107)               13803     \n",
      "=================================================================\n",
      "Total params: 134,635\n",
      "Trainable params: 134,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model \n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), recurrent_dropout=.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random \n",
    "import sys \n",
    "\n",
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = data[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        # sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 286973 samples\n",
      "Epoch 1/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.9508 - accuracy: 0.2442\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ion deep,\n",
      "All replication prompt, and reason strong,\n",
      "For h\"\n",
      "ion deep,\n",
      "All replication prompt, and reason strong,\n",
      "For he we.\n",
      " f Aoom tfne  horodeX-I\n",
      " mat  hease, antdftha. th roule it hn, Ipecit,  henewe sd wkMtl date,\n",
      "\n",
      "TI ANeEL!remerrers co   miltleltemey\n",
      " ak itce bowgwiels,  irne,, lyot  har ad see hr thor,\n",
      "\n",
      "TRO The   inde.\n",
      "G_ f se touny  ho gheyoOos s rar yerreiromhedas wier—of nid  un  he ind menf  hislabhed ,a y aMtementfe  fhy'n\n",
      "\n",
      " roy  doy wir'e ely inletfuav.\n",
      "\n",
      "IcSES.\n",
      "u2irarengem]\n",
      "[ . I\n",
      " \n",
      "286973/286973 [==============================] - 418s 1ms/sample - loss: 2.9506 - accuracy: 0.2442\n",
      "Epoch 2/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.3397 - accuracy: 0.3517\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"d him the ears of\n",
      "profiting, that what thou speakest may mo\"\n",
      "d him the ears of\n",
      "profiting, that what thou speakest may mod peim af in chet?\n",
      "Senthax, far think? mics. \n",
      "  SUDwULAVUMThe Irepiatt\n",
      "Bnt, wir hatee sero, weram.\n",
      "  Ecocoherem thist ais aish avoh, ansel\n",
      "\n",
      "I nalt stathdst\n",
      "Pay ood yor'd! Ind weish kighish?\n",
      "\n",
      "KISXUI. Piill goor aret?\n",
      "  TERCSUWthoure buul, and, bomesreend beadmigp.\n",
      "\n",
      "N PO[AIIOO. Éwriy youle, wou pevirened fosneed  pay,ar morcer, and fospe shair wave , it h this and; vichove\n",
      "Gouseton pktn?\n",
      "286973/286973 [==============================] - 403s 1ms/sample - loss: 2.3397 - accuracy: 0.3517\n",
      "Epoch 3/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.1919 - accuracy: 0.3826\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"nd jutty his confounded base,\n",
      "Swill’d with the wild and was\"\n",
      "nd jutty his confounded base,\n",
      "Swill’d with the wild and wase,\n",
      "    Prey he d ther hit  eose os nik sousu bis' necit.\n",
      "    Ol’nd shandtis for well as ince shit dand ous cands, cones mo will ist.\n",
      "  Rande ths yor carkentco sarte this bur\n",
      "         Amol t lareve be geamlond the'd and thet anking ef I ’fan, And aas pald beand in wlave not thame thaw Beren. Anfurer,\n",
      "’an my with the keve by ferlinlls saass G By coul;\n",
      "  LECAR. The weory.\n",
      "    Colver hin And yo\n",
      "286973/286973 [==============================] - 409s 1ms/sample - loss: 2.1919 - accuracy: 0.3826\n",
      "Epoch 4/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.1102 - accuracy: 0.3996\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"\n",
      "My life, sir! How, I pray? for that goes hard.\n",
      "\n",
      "\"RANIO.\n",
      "\n",
      "My life, sir! How, I pray? for that goes hard.\n",
      "\n",
      "TRANIO.\n",
      "Hove there.\n",
      "\n",
      "The wist he hy the theces leitl Sol of bingWe?\n",
      "\n",
      "    Wo lot? I Shettor and tto wan corpron. my and.\n",
      "  PELORR.\n",
      "Ang, lere it erribanBa?\n",
      "    Ene ROLIY Yous tho ringr-ith the vesens anc. I pfast whar semantr,  forr st weching bumperime pave;\n",
      "    Qxisse ste dive Vo gack bakingnog er whilk, lacs, promate,\n",
      "  Anent for ”fatse.\n",
      "    [y trit hacke at lits. I lomag if Cpthest thy buck l\n",
      "286973/286973 [==============================] - 416s 1ms/sample - loss: 2.1103 - accuracy: 0.3995\n",
      "Epoch 5/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.0529 - accuracy: 0.4120\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"his old experience the only darling,\n",
      "He bade me store up as\"\n",
      "his old experience the only darling,\n",
      "He bade me store up ast gino quthe, Corsels and the as sube dodh;\n",
      "    de? her fray em thoug to to may le\n",
      "That reals pristors; be, anote.\n",
      "\n",
      "NASTHAPT_E. Wiver.\n",
      "Sletex. Will go bece, dares fied hiod de thy the lowes dount,\n",
      "\n",
      "  Ad that the sere stedchath.\n",
      "    Gres wewlur gan wi h nis erad suapt withingh watl; do hallull?\n",
      "    Lentod and do cibrs, noce lov'the,\n",
      "Ar a dout ant of teen hest licw totd to ercerckertrs?\n",
      "\n",
      "286973/286973 [==============================] - 409s 1ms/sample - loss: 2.0529 - accuracy: 0.4120\n",
      "Epoch 6/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 2.0076 - accuracy: 0.4226\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"with heavy nothing faint and shrink.\n",
      "  BUSHY. 'Tis nothing \"\n",
      "with heavy nothing faint and shrink.\n",
      "  BUSHY. 'Tis nothing ank, thee of she\n",
      "    So mepp row? SPeartare wit read hers, fot. I st siof am.\n",
      "  SUCARTAS. Cust of her beres.\n",
      "  TORIO. To The in woun the ther Vorvenowr tuth to love,\n",
      "    Ave shut bood the gans hilf se eats Noontlin\n",
      "  ldsuld the in sior\n",
      "    Wor fome for hot thesh a bean      Frime, your Dabever,\n",
      "    The hand hor ut spastean nomat the bo than,\n",
      "    wyou lingbry you birn be shere._]\n",
      "   HEMERD\n",
      "286973/286973 [==============================] - 1108s 4ms/sample - loss: 2.0078 - accuracy: 0.4225\n",
      "Epoch 7/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 1.9708 - accuracy: 0.4311\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"But for my lord your son-\n",
      "  NORTHUMBERLAND. Why, he is dead\"\n",
      "But for my lord your son-\n",
      "  NORTHUMBERLAND. Why, he is deadrer; with you mave of with! What\n",
      "    pray, and nomer gody im beanth chound,\n",
      "    Oll thaie, of erave Go on ale. Anot L’levenf’;\n",
      "             Exeuters;\n",
      "    Ender ous hatill mantswe by\n",
      "        seindor’, hex,\n",
      "no sasd blledy?\n",
      "    Anldry of well maingur taman this mive.\n",
      "  PAINT. KIMR. I sheled, and for mayes rclean\n",
      "  hean and to gliut lligo fill.-I nat?     And diy in files in gells icony\n",
      " fre\n",
      "286973/286973 [==============================] - 402s 1ms/sample - loss: 1.9708 - accuracy: 0.4311\n",
      "Epoch 8/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 1.9407 - accuracy: 0.4387\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"\n",
      "Fear not,\n",
      "She and the Duke her husband!\n",
      "\n",
      " Enter with dru\"\n",
      "\n",
      "Fear not,\n",
      "She and the Duke her husband!\n",
      "\n",
      " Enter with drunge;\n",
      "To cranes, leteln. Enor as prood wore uat?\n",
      "\n",
      "SULOINUN.\n",
      "I hot aret, bread mos ot farter quornd,\n",
      "Faicoou uit notrale brike tee mores, age befulo?\n",
      "\n",
      "PESTY. Horke, soth I os the have Cance,\n",
      "I than that crave, y'; geter?\n",
      "\n",
      "THetryant ay a, are meatt.\n",
      "\n",
      "GHANDOTES. Ceckenpef with steen hive wil. Porters.\n",
      "\n",
      "HEDVISI RCATA. ANou this preeb' the [Astect; you brelle hile twour nove werncule.\n",
      "\n",
      "A\n",
      "286973/286973 [==============================] - 404s 1ms/sample - loss: 1.9406 - accuracy: 0.4387\n",
      "Epoch 9/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 1.9145 - accuracy: 0.4439\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \" reckon’d but of those\n",
      "Who worship dirty gods.\n",
      "\n",
      "IMOGEN.\n",
      "\"\n",
      " reckon’d but of those\n",
      "Who worship dirty gods.\n",
      "\n",
      "IMOGEN.\n",
      "He is fratt my the led he spanty-as ackarr cule,\n",
      "We't that it I wipl ot hear the free’s the dent brey. Sans.\n",
      "\n",
      "BRUTH IUS.\n",
      "Is the my did thy Hank shes far foce haves sbee.\n",
      "\n",
      "WIOL.\n",
      "And the sightree.\n",
      "\n",
      "[HORD. at hath mowe his all wist.\n",
      "Herst wote ther the tid all, cemate a aten or’d\n",
      "Theak nidgean tey thee the Hor has have it thoug.\n",
      "\n",
      "HOANENO.\n",
      "Death thee pole speplante, the and me,\n",
      "The Vigh\n",
      "286973/286973 [==============================] - 410s 1ms/sample - loss: 1.9145 - accuracy: 0.4439\n",
      "Epoch 10/10\n",
      "286812/286973 [============================>.] - ETA: 0s - loss: 1.8902 - accuracy: 0.4496\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"s uncle kept,\n",
      "His uncle York;—where I first bow’d my knee\n",
      "\"\n",
      "s uncle kept,\n",
      "His uncle York;—where I first bow’d my knee\n",
      "Dot some hase a wauld foome het plort, his thou mack.\n",
      "\n",
      "CHIONDA, NIOG. Halkince foor, wile though our mace.\n",
      "\n",
      "ALCINS. HoI wrindiess in so deaing, nat gake?\n",
      "  BUHSONK, My plietour well wein shim soble, with not\n",
      "Noe you quant leavere, of mistianoatert?\n",
      "Foom, thtur no thinkecn. Connt to not is  Hen, a dreeniofss.\n",
      "Whis lame Fancly's south; you me,\n",
      "    Thishiesank!       he cepire; stous mist mu\n",
      "286973/286973 [==============================] - 490s 2ms/sample - loss: 1.8902 - accuracy: 0.4496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bbaaa2e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(X, y,\n",
    "          batch_size=514,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-U4-S3-DeepLearning",
   "language": "python",
   "name": "ds-u4-s3-deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
